name: Cirrus clouds GPU
on:
  push:
    branches:
      - "cicd"
  pull_request:
  workflow_dispatch:

jobs:
  run-matrix:
    strategy:
      fail-fast: false
      matrix:
        image: [ubuntu24.04:nvhpc25.7_openmpi5.0.8_cuda12.9]
        

    runs-on: gha-runner-cm1-gpu
    #defaults:
    #  run:
    #    shell: bash -elo pipefail {0}
        
    container:
      image: hub.k8s.ucar.edu/stormspeed/${{ matrix.image }}
      options: --gpus all
    steps:
      - uses: actions/checkout@v4

      - name: Build and run 
        run: |
          ################################
          # Set up environment variables #
          ################################
          . /opt/spack/share/spack/setup-env.sh
          #cat /opt/spack/share/spack/setup-env.sh
          spack load python libxml2 openmpi parallel-netcdf parallelio esmf cmake
          #if ! command -v cmake &> /dev/null; then
          #  spack load cmake
          #fi
          export MPI_ROOT=$(spack location -i openmpi@5.0.8)
          export NETCDF_C_PATH=$(spack location -i netcdf-c@4.9.2)
          export NETCDF_FORTRAN_PATH=$(spack location -i netcdf-fortran@4.6.1)
          export PNETCDF=$(spack location -i parallel-netcdf@1.14.0)
          export PIO=$(spack location -i parallelio@2.6.6)
          export ESMFMKFILE=$(spack location -i esmf@8.8.1)/lib/esmf.mk
          export LAPACK=$(spack location -i netlib-lapack@3.12.1)
          if [ "${{ matrix.compiler }}" != "nvhpc" ]; then
            spack load cuda
            export CUDA_ROOT=$(spack location -i cuda@12.9.0)
          fi
          export PIO_VERSION_MAJOR=2
          export PIO_TYPENAME_VALID_VALUES="netcdf, pnetcdf, netcdf4c, netcdf4p"
          export USER=robot
          
      - name: Build and Run CM1
        run: |
          cd src/
          make -j 4 FC=nvfortran USE_OPENACC=true USE_MPI=true
          cd ../run
          mpiexec -n 4 --allow-run-as-root ./cm1.exe

